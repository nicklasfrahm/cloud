rook-ceph-cluster:
  operatorNamespace: "platform"

  cephClusterSpec:
    # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
    # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
    # tolerate taints with a key of 'storage-node'.
    placement:
      all:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists

    resources:
      mgr:
        limits:
          memory: "512Mi"
        requests:
          cpu: "250m"
          memory: "512Mi"
      mon:
        limits:
          memory: "1Gi"
        requests:
          cpu: "250m"
          memory: "1Gi"
      osd:
        limits:
          memory: "2Gi"
        requests:
          cpu: "250m"
          memory: "2Gi"
      prepareosd:
        requests:
          cpu: "100m"
          memory: "50Mi"
      mgr-sidecar:
        limits:
          memory: "40Mi"
        requests:
          cpu: "100m"
          memory: "40Mi"
      cleanup:
        limits:
          memory: "100Mi"
        requests:
          cpu: "100m"
          memory: "100Mi"
      exporter:
        limits:
          memory: "50Mi"
        requests:
          cpu: "50m"
          memory: "50Mi"
    storage:
      useAllNodes: false
      useAllDevices: false
      config:
        # the default value for this option is "false"
        encryptedDevice: "true"
      nodes:
        - name: alpaca
          devices:
            - name: "sdb"
        - name: bison
          devices:
            - name: "sdb"
        - name: camel
          devices:
            - name: "sdb"

  # -- Enable an HTTPRoute for the ceph-dashboard
  route:
    dashboard:
      host:
        name: ceph.cph02.nicklasfrahm.dev
        pathType: PathPrefix
      parentRefs:
        - namespace: platform
          name: shared-http

  # We want to use erasure coding for block storage pools.
  cephBlockPools:
    - name: ceph-blockpool
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
      spec:
        failureDomain: host
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
      storageClass:
        enabled: true
        name: ceph-block
        annotations: {}
        labels: {}
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        mountOptions: []
        # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
        allowedTopologies: []
        #        - matchLabelExpressions:
        #            - key: rook-ceph-role
        #              values:
        #                - storage-node
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
        parameters:
          # (optional) mapOptions is a comma-separated list of map options.
          # For krbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
          # For nbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
          # mapOptions: lock_on_read,queue_depth=1024

          # (optional) unmapOptions is a comma-separated list of unmap options.
          # For krbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
          # For nbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
          # unmapOptions: force

          # RBD image format. Defaults to "2".
          imageFormat: "2"

          # RBD image features, equivalent to OR'd bitfield value: 63
          # Available for imageFormat: "2". Older releases of CSI RBD
          # support only the `layering` feature. The Linux kernel (KRBD) supports the
          # full feature complement as of 5.4
          imageFeatures: layering

          # These secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-publish-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-publish-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4

  # -- A list of CephFileSystem configurations to deploy
  # @default -- See [below](#ceph-file-systems)
  cephFileSystems:
    - name: ceph-filesystem
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
          - name: default
            failureDomain: host
            replicated:
              size: 3
          - name: erasurecoded
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            limits:
              memory: "1Gi"
            requests:
              cpu: "100m"
              memory: "1Gi"
          priorityClassName: system-cluster-critical
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-filesystem
        # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        annotations: {}
        labels: {}
        mountOptions: []
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
        parameters:
          # The secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-publish-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-publish-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4

  # -- A list of CephObjectStore configurations to deploy
  # @default -- See [below](#ceph-object-stores)
  cephObjectStores:
    - name: ceph-objectstore
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            bulk: "true"
        preservePoolsOnDelete: true
        gateway:
          port: 80
          resources:
            limits:
              memory: "500Mi"
            requests:
              cpu: "100m"
              memory: "500Mi"
          instances: 2
          priorityClassName: system-cluster-critical
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: "Immediate"
        annotations: {}
        labels: {}
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
        parameters:
          # note: objectStoreNamespace and objectStoreName are configured by the chart
          region: cph02
      # Enable an HTTPRoute for the ceph-objectstore
      route:
        enabled: true
        host:
          name: s3.cph02.nicklasfrahm.dev
          pathType: PathPrefix
        parentRefs:
          - namespace: platform
            name: shared-http
